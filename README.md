# Knowledge-Distillation-Stanford40-and-CIFAR100

This project uses the concept of [Knowledge Distillation via Attention-based Feature Matching](https://arxiv.org/abs/2102.02973) and its [pytorch implementation](https://github.com/clovaai/attention-feature-distillation) in order to transfer knowledge from a source neural network(**Teacher Network**) to a target neural network(**Student Network**) on two datasets of Stanford40 and CIFAR100
The teacher, indicates a large network that is highly regularized via pre-training, and the target network,
referred to as a student, is a smaller network for a
specific task.

[Required files and datasets](https://drive.google.com/drive/folders/1U8pX87HGUYdhyCAtsvWpe4Ac_oHS3vwc?usp=sharing)


